{
  "id": "g22f4q",
  "timestamp": "2025-07-09T07:12:08.092Z",
  "snapshotId": "515mbs",
  "fileHashes": {
    ".snapshotignore": "5c0d3220",
    "web_scraper_project/news_scraper.py": "a73a586a",
    "requirements.txt": "4d2689bb",
    "README.md": "130d9660"
  },
  "fileContents": {
    ".snapshotignore": "# .snapshotignore - Configure files and directories to ignore in snapshot system\n# Syntax similar to .gitignore, supports glob pattern matching\n\n# ===== Snapshot system files =====\n.continue-reasoning/**\n.snapshotignore         # Ignore .snapshotignore file itself\n\n# ===== Temporary files and system files =====\n*.log\n*.tmp\n.DS_Store\nThumbs.db\n\n# ===== Build artifacts and dependencies =====\nnode_modules/**\ndist/**\nbuild/**\n__pycache__/**\n*.pyc\n*.pyo\n\n# ===== IDE and editor files =====\n.vscode/**\n.idea/**\n*.swp\n*.swo\n*~\n\n# ===== Runtime generated data files =====\n# These files are usually generated by program execution and should not be tracked by snapshot system\n*.json                  # Can be adjusted to more specific rules as needed\n*.csv\n*.xlsx\n*_output.*\n*_result.*\n*_data.*\n\n# ===== Cache files =====\n.cache/**\n*.cache\n\n# ===== Version control systems =====\n.git/**\n.svn/**\n\n# ===== Custom rules =====\n# Add project-specific ignore rules here\n# For example:\n# my_project_outputs/**\n# *.generated\n",
    "web_scraper_project/news_scraper.py": "import requests\nfrom bs4 import BeautifulSoup\nimport json\nimport time\nimport random\n\nHEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n}\n\nURL = 'https://news.ycombinator.com/'\nOUTPUT_FILE = 'news_headlines.json'\n\n\ndef fetch_headlines():\n    try:\n        response = requests.get(URL, headers=HEADERS)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        headlines = []\n        for item in soup.select('.athing')[:20]:\n            title = item.select_one('.titlelink').get_text()\n            link = item.select_one('.titlelink')['href']\n            headlines.append({'title': title, 'link': link})\n\n        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n            json.dump(headlines, f, ensure_ascii=False, indent=4)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred: {e}\")\n\n    finally:\n        time.sleep(random.uniform(1, 3))  # Random delay to prevent rate limiting\n\nif __name__ == '__main__':\n    fetch_headlines()\n",
    "requirements.txt": "requests\nbeautifulsoup4",
    "README.md": "# News Scraper\n\nThis Python script scrapes the top 20 news headlines from Hacker News and saves them to a `news_headlines.json` file.\n\n## Requirements\n- Python 3.x\n- requests\n- BeautifulSoup4\n\n## Installation\n```\npip install -r requirements.txt\n```\n\n## Usage\n```\npython news_scraper.py\n```\n\nThe results will be saved in `news_headlines.json`.\n\n## Features\n- Error handling and retry mechanism\n- User-Agent headers to avoid blocking\n- Random delay to prevent rate-limiting\n"
  },
  "metadata": {
    "totalFiles": 4,
    "creationTimeMs": 0
  }
}